{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Make the graphs a bit prettier, and bigger\n",
    "plt.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing NLTK toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting let's install the NLTK library (http://www.nltk.org/), by typing the following commands in vagrant terminal.\n",
    "\n",
    "* Install Numpy: `sudo pip install -U numpy`\n",
    "* Install NLTK: `sudo pip install -U nltk`\n",
    "\n",
    "Test that the library is installed properly by executing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the NLTK toolkit is installed, we need to install the NLTK data: \n",
    "\n",
    "`sudo python -m nltk.downloader -d /usr/share/nltk_data all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection u'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package hmm_treebank_pos_tagger to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package hmm_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package panlex_lite to\n",
      "[nltk_data]    |     /usr/share/nltk_data...\n",
      "^CTraceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nltk/downloader.py\", line 2268, in <module>\n",
      "    halt_on_error=options.halt_on_error)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nltk/downloader.py\", line 664, in download\n",
      "    for msg in self.incr_download(info_or_id, download_dir, force):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nltk/downloader.py\", line 543, in incr_download\n",
      "    for msg in self.incr_download(info.children, download_dir, force):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nltk/downloader.py\", line 529, in incr_download\n",
      "    for msg in self._download_list(info_or_id, download_dir, force):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nltk/downloader.py\", line 572, in _download_list\n",
      "    for msg in self.incr_download(item, download_dir, force):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nltk/downloader.py\", line 549, in incr_download\n",
      "    for msg in self._download_package(info, download_dir, force):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nltk/downloader.py\", line 618, in _download_package\n",
      "    s = infile.read(1024*16) # 16k blocks.\n",
      "  File \"/usr/lib/python2.7/socket.py\", line 380, in read\n",
      "    data = self._sock.recv(left)\n",
      "  File \"/usr/lib/python2.7/httplib.py\", line 573, in read\n",
      "    s = self.fp.read(amt)\n",
      "  File \"/usr/lib/python2.7/socket.py\", line 380, in read\n",
      "    data = self._sock.recv(left)\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sudo python -m nltk.downloader -d /usr/share/nltk_data all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra NLTK resources\n",
    "\n",
    "NLTK also comes with some of the files from Project Gutenberg already included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice  = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
    "len(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is one thing to automatically detect that a particular word occurs in a text, and to display some words that appear in the same context. However, we can also determine the location of a word in the text: how many words from the beginning it appears. This positional information can be displayed using a dispersion plot. Each stripe represents an instance of a word, and each row represents the entire text. You can produce this plot as shown below. You might like to try more words (e.g., liberty, constitution), and different texts. Can you predict the dispersion of a word before you view it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Text4 is the inauguration addresses\n",
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\", \"world\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* Pick your own book and create a dispersion plot for your keywords of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "alice = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
    "t_alice = nltk.Text(alice)\n",
    "t_alice.dispersion_plot([\"rabbit\", \"Alice\", \"hole\", \"Queen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text7.dispersion_plot([\"she\", \"he\", \"He\", \"She\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Text: Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by fetching a piece of text. We will go to [Project Gutenberg](https://www.gutenberg.org/) and fetch the text for \"The origin of species\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# The origin of species\n",
    "# Original at http://www.gutenberg.org/cache/epub/1228/pg1228.txt but there seems to be \n",
    "# some problem with downloading from Amazon EC2\n",
    "# url = \"http://www.gutenberg.org/cache/epub/1228/pg1228.txt\"\n",
    "url = \"https://dl.dropboxusercontent.com/u/16006464/DwD_Winter2015/1228.txt\"\n",
    "\n",
    "# Get the URL, do not check the SSL certificate\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Get the text\n",
    "content = resp.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951128\n"
     ]
    }
   ],
   "source": [
    "# The text contains template stuff at the beginning and end. Let's get rid of these\n",
    "start_phrase = \"*** START OF THIS PROJECT GUTENBERG EBOOK ON THE ORIGIN OF SPECIES ***\"\n",
    "end_phrase = \"*** END OF THIS PROJECT GUTENBERG EBOOK ON THE ORIGIN OF SPECIES ***\"\n",
    "s = content.index(start_phrase)\n",
    "e = content.index(end_phrase)\n",
    "true_content = content[s+len(start_phrase):e]\n",
    "\n",
    "# Approximate bytes of text\n",
    "print len(true_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency distributions, Zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our first text ready to be analyzed. Let's first do some analysis of the words that appear in this classic text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 13908 samples and 155443 outcomes>\n"
     ]
    }
   ],
   "source": [
    "tokens = true_content.split()\n",
    "\n",
    "# The nltk.Text object will offer us many useful functions for text analysis\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "# Frequency analysis for words of interest\n",
    "fdist = text.vocab()\n",
    "\n",
    "# Number of unique and total words in the text\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the frequencies of some words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fdist[\"species\"]\n",
    "print fdist[\"sexual\"]\n",
    "print fdist[\"origin\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual words of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's see a few more words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, that is not very useful. These are all words that are needed by every single English text. Only the world \"species\" seems to have some meaning. The rest of the words tell us nothing about the text; they're just English \"plumbing.\"\n",
    "\n",
    "What proportion of the text is taken up with such words? We can generate a cumulative frequency plot for these words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 50 words account for nearly half the book! (If you rememeber, we had 155443 words in the book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the frequent words don't help us, how about the words that occur once only, the so-called hapaxes? View them by typing `fdist.hapaxes()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist.hapaxes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we have a problem. We generated the words of the text by doing a simple `split()`. So our \"words\" also contain punctuation, and words with different capitalization are considered difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in order to to proper analysis we need to remove from the document all the punctuation. However, keeping only alphanumeric characters will break things like `B.Sc.` `N.Y.U.` and so on. The process of properly splitting the document into appropriate basic elements is called `tokenization`.\n",
    "\n",
    "NLTK gives us a (set of ) function call(s) that can do the tokenization (see also http://www.nltk.org/_modules/nltk/tokenize.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = '''Good bagels cost $2.88 in New York.  \n",
    "    Hey Prof. Ipeirotis, please buy me two of them.\n",
    "    \n",
    "    Thanks.\n",
    "    \n",
    "    PS: You have a Ph.D., you can handle this, right?'''\n",
    "\n",
    "print nltk.word_tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = \"On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.\"\n",
    "print nltk.word_tokenize(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s2 = \"\\\"We beat some pretty good teams to get here,\\\" Slocum said.\"\n",
    "print nltk.word_tokenize(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3 = \"Well, we couldn't have this predictable, cliche-ridden, \\\"Touched by an Angel\\\" (a show creator John Masius worked on) wanna-be if she didn't.\"\n",
    "print nltk.word_tokenize(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s4 = \"I cannot cannot work under these conditions!\"\n",
    "print nltk.word_tokenize(s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s5 = \"The company spent $30,000,000 last year.\"\n",
    "print nltk.word_tokenize(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s6 = \"The company spent 40.75% of its income last year.\"\n",
    "print nltk.word_tokenize(s6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s7 = \"He arrived at 3:00 pm.\"\n",
    "print nltk.word_tokenize(s7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s8 = \"I bought these items: books, pencils, and pens.\"\n",
    "print nltk.word_tokenize(s8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s9 = \"Though there were 150, 100 of them were old.\"\n",
    "print nltk.word_tokenize(s9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s10 = \"There were 300,000, but that wasn't enough.\"\n",
    "print nltk.word_tokenize(s10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's repeat the process now for our original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We tokenize and we also convert to lowercase for further normalization\n",
    "tokens = nltk.word_tokenize(true_content.lower())\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "# Frequency analysis for words of interest\n",
    "fdist = text.vocab()\n",
    "\n",
    "# Number of unique and total words in the text\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went from `13908 samples and 155443 outcomes` to `7687 samples and 175682 outcomes`. In other words, we have now 7687 unique tokens, and a set of 175682 tokens, as punctuation characters are now separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fdist.hapaxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len (fdist.hapaxes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So out of the 7687 unique words, 2666 of them appear only once in the text. But these are only 2666 out of the total of 175682 words in the text. This is ~1.5% of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization process can also work on separating sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = '''Good bagels cost $2.88 in N.Y.C. Hey Prof. Ipeirotis, please buy me two of them.\n",
    "    \n",
    "    Thanks.\n",
    "    \n",
    "    PS: You have a Ph.D. you can handle this, right?'''\n",
    "\n",
    "print nltk.sent_tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipf's law says that the frequencies of words in text follow a power-law: A few words account for a big fraction of the text (the very frequent ones, usually just the \"plumping\" of English), and a large fraction of the unique vocabularly (the \"hapaxes\") appear very infrequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist.plot(100, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist.plot(100, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization: Stopwords\n",
    "\n",
    "NLTK contains a corpus of stopwords, that is, high-frequency words like `the`, `to` and `also` that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print stopwords.words('english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to remove the words in a text are in the stopwords list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mystops = []\n",
    "mystops.append('one')\n",
    "mystops.append('may')\n",
    "mystops.append('would')\n",
    "mystops.append('many')\n",
    "\n",
    "def remove_stopwords(text, hapaxes):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w.lower() for w in text \n",
    "               if w.lower() not in stopwords \\ # w should not be in NLTK stopwords\n",
    "                   and w.lower() not in mystops # w should not be in custom stop word list\n",
    "                   and w.isalpha() \\ # w should consists of letters, not numbers, not punctuation\n",
    "                   and w.lower() not in hapaxes] # w should have frequency > 1 \n",
    "    return nltk.Text(content)\n",
    "\n",
    "text_nostopwords = remove_stopwords(text, fdist.hapaxes())\n",
    "fdist_nostopwords = text_nostopwords.vocab()\n",
    "print fdist_nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fdist_nostopwords.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist_nostopwords.plot(100, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization: Stemming \n",
    "\n",
    "So far, the only normalization that we did is to convert text to lowercase before doing anything with its words, e.g. `set(w.lower() for w in text)`. By using lower(), we have normalized the text to lowercase so that the distinction between `The` and `the` is ignored. Often we want to go further than this, and strip off any affixes, a task known as **stemming**. \n",
    "\n",
    "NLTK includes several off-the-shelf stemmers, and if you ever need a stemmer you should use one of these. The Porter stemmer is a very well-known stemmer, and should suffice for most of our applications. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tok = nltk.word_tokenize(raw)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "stemmed =  [porter.stem(t) for t in tok]\n",
    "\n",
    "# This idiom concatenates all the words in a list. \n",
    "# The call is s.join(list), where we join the element \n",
    "# of the list using the string s as the concatenacting character\n",
    "print \" \".join(stemmed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization: Lemmatization\n",
    "\n",
    "A further step in the normalization process is to make sure that the resulting form is a known word in a dictionary, a task known as **lemmatization**. The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary. This additional checking process makes the lemmatizer slower than the above stemmers. Notice that it doesn't handle lying, but it converts women to woman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tok = nltk.word_tokenize(raw)\n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatized =  [wnl.lemmatize(t) for t in tok]\n",
    "print \" \".join(lemmatized) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is this WordNet? It is one of the most useful resources for anyone interested in analyzing text at a more semantic level than simply frequency counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet\n",
    "\n",
    "WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155,287 words and 117,659 synonym sets. We'll begin by looking at synonyms and how they are accessed in WordNet.\n",
    "\n",
    "####  Senses and Synonyms\n",
    "\n",
    "Consider the sentence below. If we replace the word motorcar in by automobile, the meaning of the sentence stays pretty much the same:\n",
    "\n",
    "* Benz is credited with the invention of the motorcar.\n",
    "* Benz is credited with the invention of the automobile.\n",
    "\n",
    "Since everything else in the sentence has remained unchanged, we can conclude that the words motorcar and automobile have the same meaning, i.e. they are **synonyms**. We can explore these words with the help of WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, motorcar has just one possible meaning and it is identified as car.n.01, the first noun sense of car. The entity car.n.01 is called a **synset**, or **\"synonym set\"**, a collection of synonymous words (or \"lemmas\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'car', u'auto', u'automobile', u'machine', u'motorcar']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word of a synset can have several meanings, e.g., car can also signify a train carriage, a gondola, or an elevator car. However, we are only interested in the single meaning that is common to all words of the above synset. Synsets also come with a prose definition and some example sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'he needs a car to get to work']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although definitions help humans to understand the intended meaning of a synset, the words of the synset are often more useful for our programs. To eliminate ambiguity, we will identify these words as car.n.01.automobile, car.n.01.motorcar, and so on. This pairing of a synset with a word is called a **lemma**. We can get all the lemmas for a given synset, look up a particular lemma, get the synset corresponding to a lemma, and get the \"name\" of a lemma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.01.auto'),\n",
       " Lemma('car.n.01.automobile'),\n",
       " Lemma('car.n.01.machine'),\n",
       " Lemma('car.n.01.motorcar')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemmas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('car.n.01.automobile')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('car.n.01')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile').synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'automobile'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile').name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's analyze the word `car`, which has multiple **senses** (ie., meanings of the word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma name: [u'car', u'auto', u'automobile', u'machine', u'motorcar']\n",
      "Definition: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "Examples  : [u'he needs a car to get to work']\n",
      "=======================\n",
      "Lemma name: [u'car', u'railcar', u'railway_car', u'railroad_car']\n",
      "Definition: a wheeled vehicle adapted to the rails of railroad\n",
      "Examples  : [u'three cars had jumped the rails']\n",
      "=======================\n",
      "Lemma name: [u'car', u'gondola']\n",
      "Definition: the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "Examples  : []\n",
      "=======================\n",
      "Lemma name: [u'car', u'elevator_car']\n",
      "Definition: where passengers ride up and down\n",
      "Examples  : [u'the car was on the top floor']\n",
      "=======================\n",
      "Lemma name: [u'cable_car', u'car']\n",
      "Definition: a conveyance for passengers or freight on a cable railway\n",
      "Examples  : [u'they took a cable car to the top of the mountain']\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "senses = [(s.lemma_names(), s.definition(), s.examples()) for s in wn.synsets('car')]\n",
    "for s in senses:\n",
    "    print \"Lemma name:\", s[0]\n",
    "    print \"Definition:\", s[1]\n",
    "    print \"Examples  :\", s[2]\n",
    "    print \"=======================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma name: [u'bank']\n",
      "Definition: sloping land (especially the slope beside a body of water)\n",
      "Examples  : [u'they pulled the canoe up on the bank', u'he sat on the bank of the river and watched the currents']\n",
      "=======================\n",
      "Lemma name: [u'depository_financial_institution', u'bank', u'banking_concern', u'banking_company']\n",
      "Definition: a financial institution that accepts deposits and channels the money into lending activities\n",
      "Examples  : [u'he cashed a check at the bank', u'that bank holds the mortgage on my home']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: a long ridge or pile\n",
      "Examples  : [u'a huge bank of earth']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: an arrangement of similar objects in a row or in tiers\n",
      "Examples  : [u'he operated a bank of switches']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: a supply or stock held in reserve for future use (especially in emergencies)\n",
      "Examples  : []\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: the funds held by a gambling house or the dealer in some gambling games\n",
      "Examples  : [u'he tried to break the bank at Monte Carlo']\n",
      "=======================\n",
      "Lemma name: [u'bank', u'cant', u'camber']\n",
      "Definition: a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Examples  : []\n",
      "=======================\n",
      "Lemma name: [u'savings_bank', u'coin_bank', u'money_box', u'bank']\n",
      "Definition: a container (usually with a slot in the top) for keeping money at home\n",
      "Examples  : [u'the coin bank was empty']\n",
      "=======================\n",
      "Lemma name: [u'bank', u'bank_building']\n",
      "Definition: a building in which the business of banking transacted\n",
      "Examples  : [u'the bank is on the corner of Nassau and Witherspoon']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Examples  : [u'the plane went into a steep bank']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: tip laterally\n",
      "Examples  : [u'the pilot had to bank the aircraft']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: enclose with a bank\n",
      "Examples  : [u'bank roads']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: do business with a bank or keep an account at a bank\n",
      "Examples  : [u'Where do you bank in this town?']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: act as the banker in a game or in gambling\n",
      "Examples  : []\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: be in the banking business\n",
      "Examples  : []\n",
      "=======================\n",
      "Lemma name: [u'deposit', u'bank']\n",
      "Definition: put into a bank account\n",
      "Examples  : [u'She deposits her paycheck every month']\n",
      "=======================\n",
      "Lemma name: [u'bank']\n",
      "Definition: cover with ashes so to control the rate of burning\n",
      "Examples  : [u'bank a fire']\n",
      "=======================\n",
      "Lemma name: [u'trust', u'swear', u'rely', u'bank']\n",
      "Definition: have confidence or faith in\n",
      "Examples  : [u'We can trust in God', u'Rely on your friends', u'bank on your good education', u\"I swear by my grandmother's recipes\"]\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "## your code here: Analyze the word \"bank\"\n",
    "senses = [(s.lemma_names(), s.definition(), s.examples()) for s in wn.synsets('bank')]\n",
    "for s in senses:\n",
    "    print \"Lemma name:\", s[0]\n",
    "    print \"Definition:\", s[1]\n",
    "    print \"Examples  :\", s[2]\n",
    "    print \"=======================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The WordNet Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet synsets correspond to abstract concepts, and they don't always have corresponding words in English. These concepts are linked together in a hierarchy. Some concepts are very general, such as Entity, State, Event — these are called unique beginners or root synsets. Others, such as gas guzzler and hatchback, are much more specific. A small portion of a concept hierarchy is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.nltk.org/images/wordnet-hierarchy.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyponyms\n",
    "\n",
    "WordNet makes it easy to navigate between concepts. For example, given a concept like motorcar, we can look at the concepts that are more specific; the (immediate) hyponyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "motorcar = wn.synset('car.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('ambulance.n.01')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_of_motorcar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Model_T', u'S.U.V.', u'SUV', u'Stanley_Steamer', u'ambulance', u'beach_waggon', u'beach_wagon', u'bus', u'cab', u'compact', u'compact_car', u'convertible', u'coupe', u'cruiser', u'electric', u'electric_automobile', u'electric_car', u'estate_car', u'gas_guzzler', u'hack', u'hardtop', u'hatchback', u'heap', u'horseless_carriage', u'hot-rod', u'hot_rod', u'jalopy', u'jeep', u'landrover', u'limo', u'limousine', u'loaner', u'minicar', u'minivan', u'pace_car', u'patrol_car', u'phaeton', u'police_car', u'police_cruiser', u'prowl_car', u'race_car', u'racer', u'racing_car', u'roadster', u'runabout', u'saloon', u'secondhand_car', u'sedan', u'sport_car', u'sport_utility', u'sport_utility_vehicle', u'sports_car', u'squad_car', u'station_waggon', u'station_wagon', u'stock_car', u'subcompact', u'subcompact_car', u'taxi', u'taxicab', u'tourer', u'touring_car', u'two-seater', u'used-car', u'waggon', u'wagon']\n"
     ]
    }
   ],
   "source": [
    "print sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypernyms\n",
    "\n",
    "We can also navigate up the hierarchy by visiting hypernyms. Some words have multiple paths, because they can be classified in more than one way. There are two paths between car.n.01 and entity.n.01 because wheeled_vehicle.n.01 can be classified as both a vehicle and a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar.hypernyms()\n",
    "paths = motorcar.hypernym_paths()\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'entity.n.01', u'physical_entity.n.01', u'object.n.01', u'whole.n.02', u'artifact.n.01', u'instrumentality.n.03', u'container.n.01', u'wheeled_vehicle.n.01', u'self-propelled_vehicle.n.01', u'motor_vehicle.n.01', u'car.n.01']\n"
     ]
    }
   ],
   "source": [
    "print [synset.name() for synset in paths[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'entity.n.01', u'physical_entity.n.01', u'object.n.01', u'whole.n.02', u'artifact.n.01', u'instrumentality.n.03', u'conveyance.n.03', u'vehicle.n.01', u'wheeled_vehicle.n.01', u'self-propelled_vehicle.n.01', u'motor_vehicle.n.01', u'car.n.01']\n"
     ]
    }
   ],
   "source": [
    "print [synset.name() for synset in paths[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Lexical Relations: Meronyms, Holonyms, Antonyms, Entailment\n",
    "\n",
    "Hypernyms and hyponyms are called lexical relations because they relate one synset to another. These two relations navigate up and down the \"is-a\" hierarchy. Another important way to navigate the WordNet network is from items to their components (**meronyms**) or to the things they are contained in (**holonyms**). For example, the parts of a tree are its trunk, crown, and so on; the part_meronyms(). The substance a tree is made of includes heartwood and sapwood; the substance_meronyms(). A collection of trees forms a forest; the member_holonyms():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('forest.n.01')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('united_states.n.01'), Synset('united_states_army.n.01')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"USA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('alabama.n.01'),\n",
       " Synset('alaska.n.01'),\n",
       " Synset('american_state.n.01'),\n",
       " Synset('arizona.n.01'),\n",
       " Synset('arkansas.n.01'),\n",
       " Synset('california.n.01'),\n",
       " Synset('colony.n.03'),\n",
       " Synset('colorado.n.01'),\n",
       " Synset('connecticut.n.01'),\n",
       " Synset('connecticut.n.02'),\n",
       " Synset('dakota.n.02'),\n",
       " Synset('delaware.n.04'),\n",
       " Synset('district_of_columbia.n.01'),\n",
       " Synset('east.n.03'),\n",
       " Synset('florida.n.01'),\n",
       " Synset('georgia.n.01'),\n",
       " Synset('great_lakes.n.01'),\n",
       " Synset('hawaii.n.01'),\n",
       " Synset('idaho.n.01'),\n",
       " Synset('illinois.n.01'),\n",
       " Synset('indiana.n.01'),\n",
       " Synset('iowa.n.02'),\n",
       " Synset('kansas.n.01'),\n",
       " Synset('kentucky.n.01'),\n",
       " Synset('louisiana.n.01'),\n",
       " Synset('louisiana_purchase.n.01'),\n",
       " Synset('maine.n.01'),\n",
       " Synset('maryland.n.01'),\n",
       " Synset('massachusetts.n.01'),\n",
       " Synset('michigan.n.01'),\n",
       " Synset('mid-atlantic_states.n.01'),\n",
       " Synset('midwest.n.01'),\n",
       " Synset('minnesota.n.01'),\n",
       " Synset('mississippi.n.01'),\n",
       " Synset('mississippi.n.02'),\n",
       " Synset('missouri.n.01'),\n",
       " Synset('missouri.n.02'),\n",
       " Synset('montana.n.01'),\n",
       " Synset('nebraska.n.01'),\n",
       " Synset('nevada.n.01'),\n",
       " Synset('new_england.n.01'),\n",
       " Synset('new_hampshire.n.01'),\n",
       " Synset('new_jersey.n.01'),\n",
       " Synset('new_mexico.n.01'),\n",
       " Synset('new_river.n.01'),\n",
       " Synset('new_york.n.02'),\n",
       " Synset('niagara.n.02'),\n",
       " Synset('niobrara.n.01'),\n",
       " Synset('north.n.01'),\n",
       " Synset('north_carolina.n.01'),\n",
       " Synset('north_dakota.n.01'),\n",
       " Synset('ohio.n.01'),\n",
       " Synset('ohio.n.02'),\n",
       " Synset('oklahoma.n.01'),\n",
       " Synset('oregon.n.01'),\n",
       " Synset('pacific_northwest.n.01'),\n",
       " Synset('pennsylvania.n.01'),\n",
       " Synset('rhode_island.n.01'),\n",
       " Synset('rio_grande.n.01'),\n",
       " Synset('saint_lawrence.n.02'),\n",
       " Synset('south.n.01'),\n",
       " Synset('south_carolina.n.02'),\n",
       " Synset('south_dakota.n.01'),\n",
       " Synset('sunbelt.n.01'),\n",
       " Synset('tennessee.n.01'),\n",
       " Synset('texas.n.01'),\n",
       " Synset('twin.n.03'),\n",
       " Synset('utah.n.01'),\n",
       " Synset('vermont.n.01'),\n",
       " Synset('virginia.n.01'),\n",
       " Synset('washington.n.02'),\n",
       " Synset('west.n.03'),\n",
       " Synset('west_virginia.n.01'),\n",
       " Synset('wisconsin.n.02'),\n",
       " Synset('wyoming.n.01'),\n",
       " Synset('yosemite.n.01'),\n",
       " Synset('yukon.n.01')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('united_states.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see just how intricate things can get, consider the word mint, which has several closely-related senses. We can see that mint.n.04 is part of mint.n.02 and the substance from which mint.n.05 is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'batch.n.02:', u\"(often followed by `of') a large number or amount or extent\")\n",
      "(u'mint.n.02:', u'any north temperate plant of the genus Mentha with aromatic leaves and small mauve flowers')\n",
      "(u'mint.n.03:', u'any member of the mint family of plants')\n",
      "(u'mint.n.04:', u'the leaves of a mint plant used fresh or candied')\n",
      "(u'mint.n.05:', u'a candy that is flavored with a mint oil')\n",
      "(u'mint.n.06:', u'a plant where money is coined by authority of the government')\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('mint', wn.NOUN):\n",
    "    print(synset.name() + ':', synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('mint.n.02')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('mint.n.04').part_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('mint.n.05')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('mint.n.04').substance_holonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also relationships between verbs. For example, the act of walking involves the act of stepping, so walking **entails** stepping. Some verbs have multiple entailments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('step.v.01')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('walk.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chew.v.01'), Synset('swallow.v.01')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('eat.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('arouse.v.07'), Synset('disappoint.v.01')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tease.v.03').entailments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some lexical relationships hold between lemmas, e.g., **antonymy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('demand.n.02.demand')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('supply.n.02.supply').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('linger.v.04.linger')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('rush.v.01.rush').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('inclined.a.02.inclined'), Lemma('vertical.a.01.vertical')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('horizontal.a.01.horizontal').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('legato.r.01.legato')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('staccato.r.01.staccato').antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the (numerous!) lexical relations, and the other methods defined on a synset, using dir(), for example: dir(wn.synset('harmony.n.02'))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_all_hypernyms', '_definition', '_examples', '_frame_ids', '_hypernyms', '_instance_hypernyms', '_iter_hypernym_lists', '_lemma_names', '_lemma_pointers', '_lemmas', '_lexname', '_max_depth', '_min_depth', '_name', '_needs_root', '_offset', '_pointers', '_pos', '_related', '_shortest_hypernym_paths', '_wordnet_corpus_reader', 'also_sees', 'attributes', 'causes', 'closure', 'common_hypernyms', 'definition', 'entailments', 'examples', 'frame_ids', 'hypernym_distances', 'hypernym_paths', 'hypernyms', 'hyponyms', 'instance_hypernyms', 'instance_hyponyms', 'jcn_similarity', 'lch_similarity', 'lemma_names', 'lemmas', 'lexname', 'lin_similarity', 'lowest_common_hypernyms', 'max_depth', 'member_holonyms', 'member_meronyms', 'min_depth', 'name', 'offset', 'part_holonyms', 'part_meronyms', 'path_similarity', 'pos', 'region_domains', 'res_similarity', 'root_hypernyms', 'shortest_path_distance', 'similar_tos', 'substance_holonyms', 'substance_meronyms', 'topic_domains', 'tree', 'unicode_repr', 'usage_domains', 'verb_groups', 'wup_similarity']\n"
     ]
    }
   ],
   "source": [
    "print dir(wn.synset('harmony.n.02'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic similarity\n",
    "\n",
    "We have seen that synsets are linked by a complex network of lexical relations. Given a particular synset, we can traverse the WordNet network to find synsets with related meanings. Knowing which words are semantically related is useful for indexing a collection of texts, so that a search for a general term like vehicle will match documents containing specific terms like limousine.\n",
    "\n",
    "Recall that each synset has one or more hypernym paths that link it to a root hypernym such as entity.n.01. Two synsets linked to the same root may have several hypernyms in common. If two synsets share a very specific hypernym — one that is low down in the hypernym hierarchy — they must be closely related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('baleen_whale.n.01')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(minke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('whale.n.02')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(orca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('vertebrate.n.01')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(tortoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we know that whale is very specific (and baleen whale even more so), while vertebrate is more general and entity is completely general. We can quantify this concept of generality by looking up the depth of each synset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwhale_minke = right.lowest_common_hypernyms(minke)\n",
    "rwhale_minke[0].min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwhale_orca = right.lowest_common_hypernyms(orca)\n",
    "rwhale_orca[0].min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwhale_vertebrate = right.lowest_common_hypernyms(tortoise)\n",
    "rwhale_vertebrate[0].min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwhale_novel = right.lowest_common_hypernyms(novel)\n",
    "rwhale_novel[0].min_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity measures have been defined over the collection of WordNet synsets which incorporate the above insight. For example, path_similarity assigns a score in the range 0–1 based on the shortest path that connects the concepts in the hypernym hierarchy (-1 is returned in those cases where a path cannot be found). Comparing a synset with itself will return 1. Consider the following similarity scores, relating right whale to minke whale, orca, tortoise, and novel. Although the numbers won't mean much, they decrease as we move away from the semantic space of sea creatures to inanimate objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right whale - Minke : 0.25\n",
      "Right whale - Orca : 0.166666666667\n",
      "Right whale - Tortoise : 0.0769230769231\n",
      "Right whale - Novel : 0.0434782608696\n"
     ]
    }
   ],
   "source": [
    "print \"Right whale - Minke :\", right.path_similarity(minke)\n",
    "print \"Right whale - Orca :\", right.path_similarity(orca)\n",
    "print \"Right whale - Tortoise :\", right.path_similarity(tortoise)\n",
    "print \"Right whale - Novel :\", right.path_similarity(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "* Find the meronyms of the human body\n",
    "* Iterate so that you can find all the meronyms of the meronyms, and so on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('homo.n.02'),\n",
       " Synset('human.a.01'),\n",
       " Synset('human.a.02'),\n",
       " Synset('human.a.03')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human = wn.synset('homo.n.02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('arm.n.01'),\n",
       " Synset('body_hair.n.01'),\n",
       " Synset('face.n.01'),\n",
       " Synset('foot.n.01'),\n",
       " Synset('hand.n.01'),\n",
       " Synset('human_body.n.01'),\n",
       " Synset('human_head.n.01'),\n",
       " Synset('loin.n.02'),\n",
       " Synset('mane.n.02')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('arm.n.01')\n",
      "Synset('arm_bone.n.01')\n",
      "Synset('biceps_brachii.n.01')\n",
      "Synset('brachial_artery.n.01')\n",
      "Synset('cephalic_vein.n.01')\n",
      "Synset('elbow.n.01')\n",
      "Synset('funny_bone.n.01')\n",
      "Synset('musculus_articularis_cubiti.n.01')\n",
      "Synset('forearm.n.01')\n",
      "Synset('accessory_cephalic_vein.n.01')\n",
      "Synset('anconeous_muscle.n.01')\n",
      "Synset('basilic_vein.n.01')\n",
      "Synset('radial_vein.n.01')\n",
      "Synset('radius.n.04')\n",
      "Synset('ulna.n.01')\n",
      "Synset('olecranon.n.01')\n",
      "Synset('ulnar_vein.n.01')\n",
      "Synset('hand.n.01')\n",
      "Synset('ball.n.10')\n",
      "Synset('digital_arteries.n.01')\n",
      "Synset('finger.n.01')\n",
      "Synset('fingernail.n.01')\n",
      "Synset('fingertip.n.01')\n",
      "Synset('knuckle.n.01')\n",
      "Synset('pad.n.07')\n",
      "Synset('intercapitular_vein.n.01')\n",
      "Synset('metacarpal_artery.n.01')\n",
      "Synset('metacarpal_vein.n.01')\n",
      "Synset('metacarpus.n.01')\n",
      "Synset('metacarpal.n.01')\n",
      "Synset('palm.n.01')\n",
      "Synset('thenar.n.01')\n",
      "Synset('humerus.n.01')\n",
      "Synset('deltoid_tuberosity.n.01')\n",
      "Synset('triceps_brachii.n.01')\n",
      "Synset('ulnar_nerve.n.01')\n",
      "Synset('wrist.n.01')\n",
      "Synset('carpal_bone.n.01')\n",
      "Synset('carpal_tunnel.n.01')\n",
      "Synset('body_hair.n.01')\n",
      "Synset('face.n.01')\n",
      "Synset('beard.n.01')\n",
      "Synset('mustache.n.01')\n",
      "Synset('brow.n.01')\n",
      "Synset('trichion.n.01')\n",
      "Synset('cheek.n.01')\n",
      "Synset('buccal_artery.n.01')\n",
      "Synset('cheek_muscle.n.01')\n",
      "Synset('chin.n.01')\n",
      "Synset('goatee.n.01')\n",
      "Synset('eye.n.01')\n",
      "Synset('aperture.n.02')\n",
      "Synset('canthus.n.01')\n",
      "Synset('central_artery_of_the_retina.n.01')\n",
      "Synset('choroid.n.01')\n",
      "Synset('ciliary_artery.n.01')\n",
      "Synset('ciliary_body.n.01')\n",
      "Synset('conjunctiva.n.01')\n",
      "Synset('cornea.n.01')\n",
      "Synset('epicanthus.n.01')\n",
      "Synset('eyeball.n.01')\n",
      "Synset('eyelid.n.01')\n",
      "Synset('conjunctiva.n.01')\n",
      "Synset('eyelash.n.01')\n",
      "Synset('iris.n.02')\n",
      "Synset('pupil.n.02')\n",
      "Synset('lacrimal_apparatus.n.01')\n",
      "Synset('lacrimal_duct.n.01')\n",
      "Synset('lacrimal_gland.n.01')\n",
      "Synset('lacrimal_sac.n.01')\n",
      "Synset('lacrimal_artery.n.01')\n",
      "Synset('lacrimal_vein.n.01')\n",
      "Synset('lens.n.04')\n",
      "Synset('lens_capsule.n.01')\n",
      "Synset('lens_cortex.n.01')\n",
      "Synset('nictitating_membrane.n.01')\n",
      "Synset('ocular_muscle.n.01')\n",
      "Synset('pupillary_sphincter.n.01')\n",
      "Synset('retina.n.01')\n",
      "Synset('blind_spot.n.02')\n",
      "Synset('cone.n.04')\n",
      "Synset('iodopsin.n.01')\n",
      "Synset('fovea.n.01')\n",
      "Synset('macula.n.02')\n",
      "Synset('neuroepithelium.n.01')\n",
      "Synset('parafovea.n.01')\n",
      "Synset('rod.n.05')\n",
      "Synset('visual_purple.n.01')\n",
      "Synset('visual_cell.n.01')\n",
      "Synset('sclera.n.01')\n",
      "Synset('uvea.n.01')\n",
      "Synset('uveoscleral_pathway.n.01')\n",
      "Synset('eyebrow.n.01')\n",
      "Synset('venae_palpebrales.n.01')\n",
      "Synset('facial.n.01')\n",
      "Synset('facial_muscle.n.01')\n",
      "Synset('facial_vein.n.01')\n",
      "Synset('feature.n.02')\n",
      "Synset('jaw.n.02')\n",
      "Synset('jowl.n.02')\n",
      "Synset('mouth.n.02')\n",
      "Synset('lingual_artery.n.01')\n",
      "Synset('lingual_vein.n.01')\n",
      "Synset('lip.n.01')\n",
      "Synset('labial_artery.n.01')\n",
      "Synset('labial_vein.n.02')\n",
      "Synset('mouth.n.01')\n",
      "Synset('buccal_cavity.n.01')\n",
      "Synset('dentition.n.02')\n",
      "Synset('gingiva.n.01')\n",
      "Synset('palate.n.01')\n",
      "Synset('hard_palate.n.01')\n",
      "Synset('soft_palate.n.01')\n",
      "Synset('uvula.n.01')\n",
      "Synset('tastebud.n.01')\n",
      "Synset('neuroepithelium.n.01')\n",
      "Synset('taste_cell.n.01')\n",
      "Synset('salivary_gland.n.01')\n",
      "Synset('saliva.n.01')\n",
      "Synset('tongue.n.01')\n",
      "Synset('tastebud.n.01')\n",
      "Synset('neuroepithelium.n.01')\n",
      "Synset('taste_cell.n.01')\n",
      "Synset('nose.n.01')\n",
      "Synset('bridge.n.04')\n",
      "Synset('nasal.n.02')\n",
      "Synset('rhinion.n.01')\n",
      "Synset('ethmoidal_artery.n.01')\n",
      "Synset('internasal_suture.n.01')\n",
      "Synset('nasal_cavity.n.01')\n",
      "Synset('neuroepithelium.n.01')\n",
      "Synset('nostril.n.01')\n",
      "Synset('turbinate_bone.n.01')\n",
      "Synset('foot.n.01')\n",
      "Synset('arcuate_artery.n.01')\n",
      "Synset('big_toe.n.01')\n",
      "Synset('digital_arteries.n.01')\n",
      "Synset('heel.n.02')\n",
      "Synset('achilles_tendon.n.01')\n",
      "Synset('heelbone.n.01')\n",
      "Synset('instep.n.01')\n",
      "Synset('intercapitular_vein.n.01')\n",
      "Synset('little_toe.n.01')\n",
      "Synset('metatarsal_artery.n.01')\n",
      "Synset('metatarsal_vein.n.01')\n",
      "Synset('sole.n.03')\n",
      "Synset('ball.n.10')\n",
      "Synset('toe.n.01')\n",
      "Synset('tiptoe.n.01')\n",
      "Synset('toenail.n.01')\n",
      "Synset('hand.n.01')\n",
      "Synset('ball.n.10')\n",
      "Synset('digital_arteries.n.01')\n",
      "Synset('finger.n.01')\n",
      "Synset('fingernail.n.01')\n",
      "Synset('fingertip.n.01')\n",
      "Synset('knuckle.n.01')\n",
      "Synset('pad.n.07')\n",
      "Synset('intercapitular_vein.n.01')\n",
      "Synset('metacarpal_artery.n.01')\n",
      "Synset('metacarpal_vein.n.01')\n",
      "Synset('metacarpus.n.01')\n",
      "Synset('metacarpal.n.01')\n",
      "Synset('palm.n.01')\n",
      "Synset('thenar.n.01')\n",
      "Synset('human_body.n.01')\n",
      "Synset('human_head.n.01')\n",
      "Synset('countenance.n.03')\n",
      "Synset('occiput.n.01')\n",
      "Synset('pate.n.02')\n",
      "Synset('scalp.n.01')\n",
      "Synset('sinciput.n.01')\n",
      "Synset('loin.n.02')\n",
      "Synset('mane.n.02')\n"
     ]
    }
   ],
   "source": [
    "def print_meronyms(synset):\n",
    "    meronyms = synset.part_meronyms()\n",
    "    if len(meronyms) == 0:\n",
    "        return\n",
    "    for part in meronyms:\n",
    "        print part\n",
    "        print_meronyms(part)\n",
    "        \n",
    "human = wn.synset('homo.n.02')\n",
    "print_meronyms(human)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some additional, semi-random examples\n",
    "\n",
    "Below are a few examples of what we can do with the NLTK toolkit, but we will discuss the details of these later in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 914 matches:\n",
      " ranging, much diffused, and common species vary most. Species of the larger ge\n",
      "used, and common species vary most. Species of the larger genera in any country\n",
      "a in any country vary more than the species of the smaller genera. Many of the \n",
      " of the smaller genera. Many of the species of the larger genera resemble varie\n",
      " same species; often severe between species of the same genus. The relation of \n",
      "condary sexual characters variable. Species of the same genus vary in an analog\n",
      "rsified habits in the same species. Species with habits widely different from t\n",
      "On their different rates of change. Species once lost do not reappear. Groups o\n",
      "nce lost do not reappear. Groups of species follow the same general rules in th\n",
      "world. On the affinities of extinct species to each other and to living species\n",
      "ht come to the conclusion that each species had not been independently created,\n",
      " could be shown how the innumerable species inhabiting this world have been mod\n",
      " then pass on to the variability of species in a state of nature; but I shall, \n",
      "s. As many more individuals of each species are born than can possibly survive;\n",
      "y, Hybridism, or the infertility of species and the fertility of varieties when\n",
      "xplained in regard to the origin of species and varieties, if he makes due allo\n",
      " around us. Who can explain why one species ranges widely and is very numerous,\n",
      "ry numerous, and why another allied species has a narrow range and is rare? Yet\n",
      "erly entertained--namely, that each species has been independently created--is \n",
      "rroneous. I am fully convinced that species are not immutable; but that those b\n",
      "e acknowledged varieties of any one species are the descendants of that species\n",
      " than do the individuals of any one species or variety in a state of nature. Wh\n",
      "can be drawn from domestic races to species in a state of nature. I have in vai\n",
      "s and plants, and compare them with species closely allied together, we general\n",
      "from each other, and from the other species of the same genus, in several trifl\n"
     ]
    }
   ],
   "source": [
    "# Examples of the appearance of the word \"selection\"\n",
    "text.concordance(\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural selection; organic beings; natural selection,; closely allied;\n",
      "natural selection.; South America,; New Zealand,; secondary sexual;\n",
      "organic beings,; physical conditions; United States,; widely\n",
      "different; North America; closely related; one species; Dr. Hooker;\n",
      "United States; Tierra del; non facit; Glacial period,\n"
     ]
    }
   ],
   "source": [
    "# Frequent collocations in the text (usually meaningful phrases)\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dispersion plot\n",
    "text.dispersion_plot([\"selection\", \"species\", \"organic\", \"allied\", \"parent\", \"sexual\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* A frequency distribution is a collection of items along with their frequency counts (e.g., the words of a text and their frequency of appearance).\n",
    "* Tokenization is the segmentation of a text into basic units — or tokens — such as words and punctuation. Tokenization based on whitespace is inadequate for many applications because it bundles punctuation together with words. NLTK provides an off-the-shelf tokenizer nltk.word_tokenize().\n",
    "* Stemming is the process of removing the affix of a word, to create a \"normalized\" representation of the token\n",
    "* Lemmatization is a process that maps the various forms of a word (such as appeared, appears) to the canonical or citation form of the word, also known as the lexeme or lemma (e.g. appear). \n",
    "* WordNet is a semantically-oriented dictionary of English, consisting of synonym sets — or synsets — and organized into a network.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
