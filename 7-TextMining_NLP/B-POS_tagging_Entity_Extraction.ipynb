{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing and Tagging Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back in elementary school you learnt the difference between nouns, verbs, adjectives, and adverbs. These are  very useful categories for many language processing tasks. Our goals chapter is to answer the following questions:\n",
    "\n",
    "1. What are lexical categories and how are they used in natural language processing?\n",
    "2. What is a good Python data structure for storing words and their categories?\n",
    "3. How can we automatically tag each word of a text with its word class?\n",
    "\n",
    "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag to each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that and is CC, a coordinating conjunction; now and completely are RB, or adverbs; for is IN, a preposition; something is NN, a noun; and different is JJ, an adjective.\n",
    "\n",
    "NLTK provides documentation for each tag, which can be queried using the tag, e.g. `nltk.help.upenn_tagset('RB')`, or a regular expression, e.g. `nltk.help.upenn_tagset('NN.*')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset('RB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset('JJ.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another example, this time including some **homonyms**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that refuse and permit both appear as a present tense verb (VBP) and a noun (NN). E.g. refUSE is a verb meaning \"deny,\" while REFuse is a noun meaning \"trash\" (i.e. they are not homophones). Thus, we need to know which word is being used in order to pronounce the text correctly. (For this reason, text-to-speech systems usually perform POS-tagging.)\n",
    "\n",
    "See now how this information can be useful when trying to figure out the sense of a word in WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('refuse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senses = [(s.lemma_names(), s.definition(), s.examples()) for s in wn.synsets('refuse')]\n",
    "for s in senses:\n",
    "    print \"Lemma name:\", s[0]\n",
    "    print \"Definition:\", s[1]\n",
    "    print \"Examples  :\", s[2]\n",
    "    print \"=======================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is just one interpretation of _refuse_ that is a noun (garbage.n.01) and the most common interpretation of refuse as a verb means \"show unwillingness towards\" which is the correct interpretation in our context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Many words, like ski and race, can be used as nouns or verbs with no difference in pronunciation. Can you think of others? Now make up a sentence with both uses of this word, and run the POS-tagger on this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Tagged Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention in NLTK, a tagged token is represented using a **tuple** consisting of the token and the tag. We can create one of these special tuples from the standard string representation of a tagged token, using the function str2tuple():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('They', 'PRP')\n",
      "They\n",
      "PRP\n"
     ]
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "tagged = nltk.pos_tag(text)\n",
    "tagged_token = tagged[0]\n",
    "\n",
    "print tagged_token\n",
    "print tagged_token[0]\n",
    "print tagged_token[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'refuse', 'to', 'permit', 'us', 'to', 'obtain', 'the', 'refuse', 'permit']\n",
      "['They', 'refuse', 'to', 'permit', 'us', 'to', 'obtain', 'the', 'refuse', 'permit']\n",
      "['PRP', 'VBP', 'TO', 'VB', 'PRP', 'TO', 'VB', 'DT', 'NN', 'NN']\n",
      "<FreqDist with 6 samples and 10 outcomes>\n",
      "  VB   NN   TO  PRP  VBP   DT \n",
      "   2    2    2    2    1    1 \n"
     ]
    }
   ],
   "source": [
    "print text\n",
    "tokens = [a for (a, b) in tagged]\n",
    "print tokens\n",
    "tags = [b for (a, b) in tagged]\n",
    "print tags\n",
    "fd = nltk.FreqDist(tags)\n",
    "print fd\n",
    "fd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "tagged = nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NNP   NN    ,   IN  PRP    .   DT   RB   CC   VB PRP$   JJ    :  VBP  NNS  VBZ   TO   MD  VBD   ''  VBN   WP  POS  WRB  VBG NNPS  WDT   CD  JJR  JJS   EX  RBR   RP  RBS  WP$  PDT -NONE- \n",
      "6079 4567 2892 2598 2514 2363 2359 1427 1263 1256 1184 1104 1034 1009  993  692  672  561  536  503  260  254  220  210  149  105   94   89   78   76   69   65   34   25   18    5    3 \n"
     ]
    }
   ],
   "source": [
    "tokens = [a for (a, b) in tagged]\n",
    "tags = [b for (a, b) in tagged]\n",
    "fd = nltk.FreqDist(tags)\n",
    "fd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "tagged_wsj = nltk.pos_tag(text7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NN  NNP   IN   DT -NONE-  NNS   JJ    ,    .   CD  VBD   RB   VB   CC   TO  VBZ  VBN  PRP  VBG  VBP   MD  POS PRP$    $   ``   ''    :  WDT  JJR NNPS   WP   RP  JJS  WRB  RBR -RRB- -LRB-   EX  RBS  PDT    #  WP$   LS   FW   UH  SYM \n",
      "13079 9950 9645 8142 6593 6033 5760 4887 3874 3552 3039 2748 2562 2267 2180 2125 2124 1715 1455 1313  927  825  739  724  712  694  563  446  376  252  241  213  178  177  134  126  120   86   34   22   16   14    7    4    2    1 \n"
     ]
    }
   ],
   "source": [
    "tokens_wsj = [a for (a, b) in tagged_wsj]\n",
    "tags_wsj = [b for (a, b) in tagged_wsj]\n",
    "fd_wsj = nltk.FreqDist(tags_wsj)\n",
    "fd_wsj.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "\n",
    "Load a text of your choice, tokenize it, and perform part of speech tagging on it. Then extract the nouns from the text, and perform a frequency anaysis, to identify the most common nouns in the text. (Warning: POS tagging takes a good amount of time when processing long texts, so try to select a text with less than 10K tokens, or simply perform POS tagging on the first 10K-20K tokens).\n",
    "\n",
    "Repeat the exercise for adjectives.\n",
    "\n",
    "PS: If you want to parse text from HTML without resorting to XPath expressions, you can use the \"BeautifulSoup\" library as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"http://www.nytimes.com/2014/11/11/world/asia/obama-apec-china-hong-kong.html\"\n",
    "resp = requests.get(url)\n",
    "html = resp.text \n",
    "raw = BeautifulSoup(html).get_text()\n",
    "\n",
    "# The code below is to remove the junk that was extracted in addition to the article\n",
    "start = raw.index(u\"BEIJING —\")\n",
    "end = raw.index(u\"than Shanghai Tang.\")\n",
    "raw = raw[start:end]\n",
    "\n",
    "# Let's do the NLTK stuff\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "tagged = nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primitive sentiment analysis\n",
    "\n",
    "Adjectives are known to be the primary carriers of sentiment. So now let's pick a piece of text and identify the adjectives that appear in the text and their sentiment score. For that, we will use the  SentiWordNet, a lexical resource for opinion mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See http://www.nltk.org/_modules/nltk/corpus/reader/sentiwordnet.html for the documentation\n",
    "\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "print(swn.senti_synset('breakdown.n.03'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's analyze a review text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Amazon review for Samsung Galaxy S5, White 16GB\n",
    "# http://www.amazon.com/review/R3UULR1IWEUS4I/ref=cm_cr_dp_title?ie=UTF8&ASIN=B00IZ1X21K&nodeID=2335752011&store=wireless\n",
    "\n",
    "\n",
    "content = u'''\n",
    "First off, I am not a professional reviewer, nor am I employed or compensated by Samsung or any other company. Instead of boring you with facts - which you can find anywhere on the Net - I will just give you some real-world impressions on how it looks, feels, and runs. With that out of the way, let's get to the point and the nitty gritty, shall we?\n",
    "\n",
    "* THE SCREEN - that is the very first thing you will notice when you look at the S5. Samsung has found its niche with AMOLED screens, which are BRIGHT & SATURATED. Everything almost literally jumps out at you, and sometimes even too much so. I had to switch to the \"natural\" setting, as the \"vivid\" and even \"standard\" profiles are too saturated(and FAKE) for me. It's better as a demo unit to draw you in, but for everyday use, I recommend switching to the natural profile.\n",
    "FACTS: The Galaxy S5 has a 5.1-inch Super AMOLED capacitive touchscreen with Full HD resolution - 1080 x 1920 pixels or ~432 ppi pixel density, plus Gorilla Glass 3 to protect the screen from scratches.\n",
    "\n",
    "* The Look - the S5 has a more squared-off edges look than the S4, which is more squared off than the S3, but all three are not as angular as the S2. In terms of roundness-to square-ness, it goes from the S3 - S4 - S5 - S2 (the original S just looks like an iPhone 3GS). Check out my images for an easier comparison. The S5 is the tallest and widest, but not the thickest of the Galaxy S's. The best thing I can say about this is it's an evolution. Beauty is subjective, so judge for yourself. The front side is almost the same as any other Galaxy phone: You have the physical Home button, flanked by the \"back\" and \"menu\" capacitive buttons. Probably the most improved aspect of the design is in its functionality - it is now dust-proof, and water-proof up to 3 feet!\n",
    "FACTS: The dimensions are 5.59\" x 2.85\" x 0.32\"(142cm x 72.5cm x 8.1cm), and weighs 5.11oz(145g).\n",
    "\n",
    "* The Feel - Samsung has taken a lot of flack for making the Galaxy S line so cheap looking and feeling with its plastic bodies, for being the top Android phone maker. HTC has been known to have the best craftsmanship with their all-metal One phones. Perhaps Samsung feel they are so dominant that they don't have to spend more to mass-produce metal phones, but since they don't want to come off as too arrogant, so their compromise is a dimpled, faux-rubber backside like the Nexus 7(2012) and its very own Galaxy Note 3. It definitely gives a better feel - it doesn't slip and slide in your hands or pockets anymore - but it cannot compare to the feel and craftsmanship of the HTC One(both the m7 and m8). It is on the right track though, so let's hope that rumored luxury \"F\" line or next year's S6 will continue to get better.\n",
    "\n",
    "* How it Runs - This phone is fast, fast, FAST! With a 2.5gHz Snapdragon 801, it has the fastest processor out there right now. It terms of real speed, I cannot say if it is faster than the HTC One m8 or the Sony Xperia Z2, but it is definitely up there. When you touch an app icon to launch it, it launches nearly instantly. To really see how this phone flies, just open the gallery app and scroll through all your photos and you'll see what I mean. Usually the gallery is where most phones stutter as it tries to load all your photos and albums - but NOT the S5!\n",
    "\n",
    "* The Camera - FINALLY! Samsung has decided to make a decent camera, and not just as an afterthought. This 16mp camera is really awesome, so much better than the S4. I would always get washed out images with my S3/S4/Note 2, but with the S5, it actually looks like it's from a decent point-and-shoot dedicated camera with crisp, bright, and saturated images. Low-light shooting is also vastly improved, although not as good as the new HTC One m8. 16mp means 5312 x 2988 -resolution images, so you can actually blow them up or crop them down without fearing the dreaded pixelation monster. There are a myriad of other cool and useful camera features that I will save for you to find out(like macro and \"Google Street View\" modes :]). And lastly, the focus is quick, quick, QUICK! Nearly instantaneous focus allows you to capture those hard-to-capture moments easier. A definitely thumbs up to Samsung for paying attention to the camera and its functions.\n",
    "\n",
    "* Software - I'm still trying to figure out everything, as there is A LOT of stuff under the hood. Samsung's TouchWiz user interface this time around is A LOT less intrusive though, as much as can be without being totally stock Android, I guess. The layout and iconography are flatter and simpler, and for the better in my view. There is also a new sensor on the back, just beneath the camera lens. It is a heart-rate monitor/pedometer, and it comes with its own health app called S Health. There is a new battery-saving mode which can save you precious minutes when you're caught in a bind. All in all, I think this version is a lot nicer-looking, more responsive, and better than the precious S phones.\n",
    "\n",
    "The ultimate question is whether this phone is a worthy upgrade over the S4. As my review title suggests, it is an evolution, an incremental upgrade over the S4. So with that said I cannot whole-heartedly recommend it if you already have a good phone, or even over the S4. But I do feel this upgrade is more vast and much better than from the S3 to the S4, so in that sense Samsung has done a much better job this year. If you are switching from an older phone that was made at least 2 years ago, then I would tell you jump right in and try the S5 - it will not disappoint you. But for those with already a good phone, and/or say you just finished year one of your 2-year contract, then I would say think hard before you make the leap. For my money, I think the Note 4 and S6 will be the bigger upgrades more worth waiting for.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(content)\n",
    "text = nltk.Text(tokens)\n",
    "tagged = nltk.pos_tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's keep the adjectives only\n",
    "adjectives = [w for (w,t) in tagged if t=='JJ']\n",
    "print adjectives \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we want to use WordNet and eliminate the words that do not appear in our lexicon\n",
    "# Since we do not have much of information for further disambiguation, we will keep only the \n",
    "# most popular interpretation (list element 0) if there are multiple ones\n",
    "resolved_adjectives = [(w, swn.senti_synsets(w, 'a')[0]) for w in adjectives if len(swn.senti_synsets(w, 'a'))>0]\n",
    "print resolved_adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SentiWordNet assigns to each synset of WordNet three\n",
    "# sentiment scores: positivity, negativity, and objectivity.\n",
    "\n",
    "for (w,a) in resolved_adjectives:\n",
    "    print \"Word:\", w\n",
    "    print \"Synset:\", a\n",
    "    print \"Pos score:\",  a.pos_score()\n",
    "    print \"Neg score:\",  a.neg_score()\n",
    "    print \"Objectivity score:\",  a.obj_score()\n",
    "    print \"======================================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But let's take a look at what we rejected\n",
    "rejected_adjectives = [w for w in adjectives if len(swn.senti_synsets(w, 'a'))==0]\n",
    "print rejected_adjectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we would also like to figure out what the adjectives in the text refer to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(tagged)):\n",
    "    current_word = tagged[i][0]\n",
    "    current_pos = tagged[i][1]\n",
    "    if current_pos == 'NN':\n",
    "        previous_word = tagged[i-1][0]\n",
    "        previous_pos = tagged[i-1][1]\n",
    "        if previous_pos == 'JJ':\n",
    "            print previous_word + \" \" + current_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 1\n",
    "\n",
    "Try a new text with this type of sentiment analysis. Let's figure out what works and what does not\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Instead of adjectives-nouns, we can instead use adverbs and verbs (e.g., \"works nicely\"). Let's modify the code above to extract patterns involving verbs and adverbs\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "How can you modify the code to find more patterns, instead of just JJ-NN (adjective followed by noun)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "Named entities are definite noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on. Here is a list of some commonly used Named Entities:\n",
    " \n",
    "* ORGANIZATION\t(e.g., Georgia-Pacific Corp., WHO)\n",
    "* PERSON\t(e.g., Eddy Bonte, President Obama)\n",
    "* LOCATION\t(e.g., Murray River, Mount Everest)\n",
    "* DATE\t(e.g., June, 2008-06-29)\n",
    "* TIME\t(e.g., two fifty a m, 1:30 p.m.)\n",
    "* MONEY\t(e.g., 175 million Canadian Dollars, GBP 10.40)\n",
    "* PERCENT\t(e.g., twenty pct, 18.75 %)\n",
    "* FACILITY\t(e.g., Washington Monument, Stonehenge)\n",
    "* GPE\t(e.g., South East Asia, Midlothian)\n",
    "\n",
    "The goal of a named entity recognition (NER) system is to identify all textual mentions of the named entities. This can be broken down into two sub-tasks: identifying the boundaries of the NE, and identifying its type.\n",
    "\n",
    "NLTK provides a module that has already been trained to recognize named entities, accessed with the function nltk.ne_chunk(). If we set the parameter `binary=True`, then named entities are just tagged as NE; otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = u'''\n",
    "Morgan Stanley received a long-awaited ratings upgrade from Moody’s Investors Service, an endorsement of the firm’s strategy shift and a move that could help the bank pick up new trading clients.\n",
    "\n",
    "In raising Morgan Stanley’s rating two notches, Moody’s determined that strategic changes at the investment bank in recent years have resulted in a safer business model and improved profitability.\n",
    "\n",
    "Goldman Sachs Group Inc., Bank of America Corp. and Citigroup Inc. received one-notch upgrades as part of a broader Moody’s review of the largest global banks.\n",
    "\n",
    "The actions were in part a reversal of the downgrades Moody’s issued to several banks in 2012. Moody’s at that time found that the European sovereign-debt crisis and other macroeconomic and regulatory factors were crimping banks’ profitability.\n",
    "\n",
    "Since the downgrade, Morgan Stanley Chairman and Chief Executive James Gorman has continued to push the bank away from volatile activities such as trading while expanding in businesses such as wealth management that generate earnings more steadily. Moody’s said that those switches to its business mix, as well as changes to its funding that would lead to fewer losses for creditors if the bank were to fail, led it to upgrade Morgan Stanley’s debt by two notches from Baa2 to A3.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Morgan/NNP)\n",
      "  (PERSON Stanley/NNP)\n",
      "  received/VBD\n",
      "  a/DT\n",
      "  long-awaited/JJ\n",
      "  ratings/NNS\n",
      "  upgrade/VBP\n",
      "  from/IN\n",
      "  (ORGANIZATION Moody\\u2019s/NNP Investors/NNPS Service/NNP)\n",
      "  ,/,\n",
      "  an/DT\n",
      "  endorsement/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  firm\\u2019s/JJ\n",
      "  strategy/NN\n",
      "  shift/NN\n",
      "  and/CC\n",
      "  a/DT\n",
      "  move/NN\n",
      "  that/WDT\n",
      "  could/MD\n",
      "  help/VB\n",
      "  the/DT\n",
      "  bank/NN\n",
      "  pick/NN\n",
      "  up/IN\n",
      "  new/JJ\n",
      "  trading/NN\n",
      "  clients/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  In/IN\n",
      "  raising/NN\n",
      "  (PERSON Morgan/NNP)\n",
      "  Stanley\\u2019s/NNP\n",
      "  rating/NN\n",
      "  two/CD\n",
      "  notches/NNS\n",
      "  ,/,\n",
      "  (PERSON Moody\\u2019s/NNP)\n",
      "  determined/VBD\n",
      "  that/IN\n",
      "  strategic/JJ\n",
      "  changes/NNS\n",
      "  at/IN\n",
      "  the/DT\n",
      "  investment/NN\n",
      "  bank/NN\n",
      "  in/IN\n",
      "  recent/JJ\n",
      "  years/NNS\n",
      "  have/VBP\n",
      "  resulted/VBN\n",
      "  in/IN\n",
      "  a/DT\n",
      "  safer/NN\n",
      "  business/NN\n",
      "  model/NN\n",
      "  and/CC\n",
      "  improved/VBN\n",
      "  profitability/NN\n",
      "  ./.)\n",
      "(S\n",
      "  (PERSON Goldman/NNP)\n",
      "  (PERSON Sachs/NNP Group/NNP Inc./NNP)\n",
      "  ,/,\n",
      "  (ORGANIZATION Bank/NNP)\n",
      "  of/IN\n",
      "  (ORGANIZATION America/NNP Corp./NNP)\n",
      "  and/CC\n",
      "  (ORGANIZATION Citigroup/NNP Inc./NNP)\n",
      "  received/VBD\n",
      "  one-notch/JJ\n",
      "  upgrades/NNS\n",
      "  as/IN\n",
      "  part/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  broader/JJR\n",
      "  Moody\\u2019s/JJ\n",
      "  review/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  largest/JJS\n",
      "  global/JJ\n",
      "  banks/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  The/DT\n",
      "  actions/NNS\n",
      "  were/VBD\n",
      "  in/IN\n",
      "  part/NN\n",
      "  a/DT\n",
      "  reversal/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  downgrades/NNS\n",
      "  Moody\\u2019s/-NONE-\n",
      "  issued/VBN\n",
      "  to/TO\n",
      "  several/JJ\n",
      "  banks/NNS\n",
      "  in/IN\n",
      "  2012/CD\n",
      "  ./.)\n",
      "(S\n",
      "  Moody\\u2019s/NNS\n",
      "  at/IN\n",
      "  that/DT\n",
      "  time/NN\n",
      "  found/VBD\n",
      "  that/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION European/NNP)\n",
      "  sovereign-debt/NNP\n",
      "  crisis/NN\n",
      "  and/CC\n",
      "  other/JJ\n",
      "  macroeconomic/JJ\n",
      "  and/CC\n",
      "  regulatory/JJ\n",
      "  factors/NNS\n",
      "  were/VBD\n",
      "  crimping/VBG\n",
      "  banks\\u2019/JJ\n",
      "  profitability/NN\n",
      "  ./.)\n",
      "(S\n",
      "  Since/IN\n",
      "  the/DT\n",
      "  downgrade/NN\n",
      "  ,/,\n",
      "  (PERSON Morgan/NNP Stanley/NNP Chairman/NNP)\n",
      "  and/CC\n",
      "  Chief/NNP\n",
      "  Executive/NNP\n",
      "  (PERSON James/NNP Gorman/NNP)\n",
      "  has/VBZ\n",
      "  continued/VBN\n",
      "  to/TO\n",
      "  push/VB\n",
      "  the/DT\n",
      "  bank/NN\n",
      "  away/RB\n",
      "  from/IN\n",
      "  volatile/JJ\n",
      "  activities/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  trading/NN\n",
      "  while/IN\n",
      "  expanding/VBG\n",
      "  in/IN\n",
      "  businesses/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  wealth/NN\n",
      "  management/NN\n",
      "  that/WDT\n",
      "  generate/NN\n",
      "  earnings/NNS\n",
      "  more/RBR\n",
      "  steadily/RB\n",
      "  ./.)\n",
      "(S\n",
      "  Moody\\u2019s/NNS\n",
      "  said/VBD\n",
      "  that/IN\n",
      "  those/DT\n",
      "  switches/NNS\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  business/NN\n",
      "  mix/NN\n",
      "  ,/,\n",
      "  as/IN\n",
      "  well/RB\n",
      "  as/IN\n",
      "  changes/NNS\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  funding/NN\n",
      "  that/WDT\n",
      "  would/MD\n",
      "  lead/VB\n",
      "  to/TO\n",
      "  fewer/JJR\n",
      "  losses/NNS\n",
      "  for/IN\n",
      "  creditors/NNS\n",
      "  if/IN\n",
      "  the/DT\n",
      "  bank/NN\n",
      "  were/VBD\n",
      "  to/TO\n",
      "  fail/VB\n",
      "  ,/,\n",
      "  led/VBN\n",
      "  it/PRP\n",
      "  to/TO\n",
      "  upgrade/VB\n",
      "  (PERSON Morgan/NNP)\n",
      "  Stanley\\u2019s/NNP\n",
      "  debt/NN\n",
      "  by/IN\n",
      "  two/CD\n",
      "  notches/NNS\n",
      "  from/IN\n",
      "  (GPE Baa2/NNP)\n",
      "  to/TO\n",
      "  (GPE A3/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(raw)\n",
    "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "for sent in sentences:\n",
    "    named_entities = nltk.ne_chunk(sent, binary=False)\n",
    "    print named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
