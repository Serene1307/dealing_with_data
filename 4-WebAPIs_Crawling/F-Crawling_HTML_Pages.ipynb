{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling and Extracting Data from Websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module uses a set of non-standard libraries, which need to be installed on your machine. By default, your instance should have these installed, but if this is not the case, type these in the Unix shell prompt\n",
    "\n",
    "`sudo apt-get install libxml2-dev libxslt-dev python-dev` \n",
    "\n",
    "and then\n",
    "\n",
    "`sudo pip install lxml`\n",
    "\n",
    "`sudo pip install pandas`\n",
    "\n",
    "It will take a few minutes to get everything installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the headlines from ESPN.com\n",
    "\n",
    "Let's start by trying to fetch the headlines from the site ESPN.com.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "import pandas # To create a dataframe\n",
    "\n",
    "# Let's start by fetching the page, and parsing it\n",
    "url = \"http://www.espn.com/\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) # parse it and create a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `doc` variable is an `HtmlElement` object, and we can now use **XPath** queries to locate the elements that we need. (Depending on time, we may do in class a tutorial on XPath. For now, you can look at the [W3Schools tutorial](http://www.w3schools.com/xpath/xpath_nodes.asp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to find all the `<a ...> ... </a>` tags in the returned html, which store the links in the page, we issue the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links = doc.findall(\".//a\")\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lnk = links[70]\n",
    "type(lnk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lnk` variable is again an HtmlElement. To get parts of the html element that we need, we can use the `get` method (e.g., to get the `href` attribute) and the `text_content` method (to get the text within the `<a>...</a>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lnk.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lnk.text_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's revisit the _list comprehension_ approach that we discussed in the Python Primer session, for quickly constructing lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = [lnk.get(\"href\") for lnk in doc.findall('.//a')]\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Use a list compresension approach, to get the text_content of all the URLs in the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now create a list where we put together text content and the URL for each link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get the headlines..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine how we can get the data from the website. The key is to understand the structure of the HTML, where the data that we need is stored, and how to fetch the elements. Then, using a combination of `find` and `findall` commands, together with the appropriate XPath queries, we will get what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "import pandas # To create a dataframe\n",
    "\n",
    "# Let's start by fetching the page, and parsing it\n",
    "url = \"http://www.espn.com/\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) # parse it and create a document\n",
    "\n",
    "# By using the \"Right-Click > Inspect\" option of Chrome,\n",
    "# we right click on the headlines and select \"Inspect\".\n",
    "# This opens the source code.\n",
    "# There we see that all under a <ul class=\"headlines\"> tag\n",
    "# which is also the only such tag in the html source\n",
    "# So, we can use the \"find\" command (instead of the \"findall\", \n",
    "# which would return a list, with just a single entry in this case)\n",
    "headlineNode = doc.find('//div[@class=\"headlines\"]//a')\n",
    "\n",
    "# Each headline is under a  <li><a href=\"....\"></a> \n",
    "# tag. We also notice that there are two <a></a> tags within\n",
    "# each <li>, and we only need the first one.\n",
    "# So, we get all the <li><a ...> tags within the <ul class=\"headlines\">\n",
    "# (which is stored in the \"headlineNode\" variable)\n",
    "headlines = headlineNode.findall('./ul/li/a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we have the nodes with the conent in the headlines variable\n",
    "# We extract the text and the URL\n",
    "data = [(a.text_content(), a.get(\"href\")) for a in headlines]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And let's create our dataframe\n",
    "dataframe = pandas.DataFrame(data, columns=[\"Title\", \"URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Yahoo Finance (Advanced Example)\n",
    "\n",
    "Let's try to parse the page from Yahoo Finance that contains the Options prices for YHOO (or any other company, actually). Let's start with the standard process of fetching the URL, and parsing the content into a document/tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "\n",
    "url = \"http://finance.yahoo.com/q/op?s=YHOO+Options\"\n",
    "\n",
    "# get the html of that url\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the html into a tree\n",
    "doc = html.fromstring(response.text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the data that we want\n",
    "\n",
    "Now, let's say that we want to get the prices for the different options. Scraping HTML is a little bit of an art, and sometimes involves trial and error, as the html formatting of the page can change at any point.\n",
    "\n",
    "To see where the data is stored, we need to look at the HTML, and try to understand the structure of what surrounds our data of interest. For our case, we see that the prices are stored in an HTML `<table>` element. The table also contains the attribute `class=\"details-table quote-table Fz-m\"`. So, we fetch these two tables, and we ensure that we have two such tables (the call and put) in our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables = doc.findall('.//table[@class=\"details-table quote-table Fz-m\"]')\n",
    "len(tables)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The first table contains the calls and the second contains the puts\n",
    "calls = tables[0] \n",
    "\n",
    "puts = tables[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to again parse the content of these tables. For that, we get back a list of elements that contain \"table rows\" which are the `<tr>...</tr>` elements. Let's get these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calls_rows = calls.findall('.//tr')\n",
    "puts_rows = puts.findall('.//tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, within the `<tr>` nodes (that represent table rows) we see a set of `<td></td>` elements that contain the cells of each row. You will also notice that the first row contains `<th></th>` cells instead of `<td></td>`; the `<th></th>` cells are commonly used to mark the \"headers\" of the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you have seen how we can use list comprehensions to get back the data that we want in a list format. Now, we will write a function that will return us directly the text content of the cells in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function takes as input a \"tr\" node\n",
    "# and returns a list of the text content for each\n",
    "# of the cells. To avoid creating two functions,\n",
    "# one for header (<th>) cells and one for data (<td>) cells\n",
    "# we pass a parameter isHeader that specifies that we are \n",
    "# extracting from\n",
    "def extractCells(row, isHeader=False):\n",
    "    if isHeader:\n",
    "        cells = row.findall('.//th')\n",
    "    else:\n",
    "        cells = row.findall('.//td')\n",
    "        \n",
    "    # The ...strip().splitlines()[0] is not strictly necessary, but it helps for beautifying the headers\n",
    "    return [val.text_content().strip().splitlines()[0] for val in cells]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's get the contents of the header row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractCells(calls_rows[0], isHeader=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's get the contents of the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractCells(calls_rows[2], isHeader=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, out goal is to take all the above, and put them together. We will first create a function that takes the \"table\" object as a parameter, and creates a DataFrame object, with the headers of the table as headers for the DataFrame and the table rows become the rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "def parse_options_data(table):\n",
    "    rows = table.findall(\".//tr\")\n",
    "    header = extractCells(rows[0], isHeader=True)\n",
    "    data = [extractCells(row, isHeader=False) for row in rows[2:]]\n",
    "    return pandas.DataFrame(data, columns=header)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "from pandas.io.parsers import TextParser\n",
    "import pandas\n",
    "\n",
    "def extractCells(row, isHeader=False):\n",
    "    if isHeader:\n",
    "        cells = row.findall('.//th')\n",
    "    else:\n",
    "        cells = row.findall('.//td')\n",
    "    return [val.text_content().strip().splitlines()[0] for val in cells]\n",
    "\n",
    "def parse_options_data(table):\n",
    "    rows = table.findall(\".//tr\")\n",
    "    header = extractCells(rows[0], isHeader=True)\n",
    "    data = [extractCells(row, isHeader=False) for row in rows[2:]]\n",
    "    return pandas.DataFrame(data, columns=header)\n",
    "\n",
    "url = \"http://finance.yahoo.com/q/op?s=YHOO+Options\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) \n",
    "\n",
    "tables = doc.findall('.//table[@class=\"details-table quote-table Fz-m\"]')\n",
    "calls = tables[0] \n",
    "puts = tables[1] \n",
    "call_data = parse_options_data(calls)\n",
    "put_data = parse_options_data(puts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Render our plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make the graphs a bit prettier, and bigger\n",
    "# Do not need to understand these commands\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "\n",
    "X = call_data[\"Strike\"]\n",
    "Y = call_data[\"Last\"]\n",
    "plt.plot(X, Y, 'r-')\n",
    "\n",
    "Y = call_data[\"Bid\"]\n",
    "plt.plot(X, Y, 'g-')\n",
    "\n",
    "Y = call_data[\"Ask\"]\n",
    "plt.plot(X, Y, 'b-')\n",
    "\n",
    "\n",
    "plt.title('YHOO Calls Prices')\n",
    "plt.xlabel('Strike')\n",
    "plt.ylabel('Last/Bid/Ask Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Class Example: Crawl BuzzFeed\n",
    "\n",
    "* We will try to get the top articles that appear on Buzzfeed\n",
    "* We will grab the link for the article, the text of the title, the description, and the editor.\n",
    "* The results will be stored in a dataframe (we will see in detail what a dataframe is, in a couple of modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_article_elements(article):\n",
    "    headline = article.find(\".//a[@class='lede__link']\")\n",
    "    \n",
    "    if headline == None:\n",
    "        headline_text = None\n",
    "        return None\n",
    "    else: \n",
    "        headline_text = headline.text_content()\n",
    "\n",
    "    description = article.find(\"./p[@class='lede__kicker']\")\n",
    "    if description == None:\n",
    "        descr_text = None\n",
    "    else:\n",
    "        descr_text = description.text_content()\n",
    "    \n",
    "    editor = article.find(\".//a[@editor]\")\n",
    "    if editor == None:\n",
    "        editor_text = None\n",
    "    else:\n",
    "        editor_text = editor.text_content()\n",
    "        \n",
    "    result = {\n",
    "        'headline': headline_text,\n",
    "        'description': descr_text,\n",
    "        'editor': editor_text\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "resp = requests.get(\"http://www.buzzfeed.com\")\n",
    "doc = html.fromstring(resp.text)\n",
    "\n",
    "articles = doc.xpath(\"//div[@class='lede__body']\")\n",
    "\n",
    "data = [extract_article_elements(article) for article in articles]\n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Solution for Buzzfeed (as of March 1st, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "import pandas\n",
    "\n",
    "# Let's start by fetching the page, and parsing it\n",
    "url = \"http://www.buzzfeed.com/\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) # parse it and create a document\n",
    "\n",
    "articleNodes = doc.xpath(\".//ul[contains(@class,'grid-posts')]//div[@class='lede ']/div[@class='lede__body']\") \n",
    "\n",
    "def parseArticleNode(article):\n",
    "    headline = article.find(\".//h2[@class='lede__title lede__title--medium']/a[@class='lede__link']\")\n",
    "    if headline == None:\n",
    "        return dict()\n",
    "    headline_text = headline.text_content()[2:]\n",
    "    headline_link = headline.get(\"href\").strip()\n",
    "    \n",
    "    description = article.find(\"./p[@class='lede__kicker']\")\n",
    "    if description == None:\n",
    "        description_text = None\n",
    "    else:\n",
    "        description_text = description.text_content().strip()\n",
    "    \n",
    "    editor = article.find('.//a[@editor]') \n",
    "    if editor == None:\n",
    "        editor_text = None\n",
    "    else:\n",
    "        editor_text = editor.text_content().strip()\n",
    "    \n",
    "    result = {\n",
    "        \"headline\": headline_text,\n",
    "        \"URL\" : headline_link,\n",
    "        \"description\" : description_text,\n",
    "        \"editor\" : editor_text\n",
    "    }\n",
    "    return result\n",
    "\n",
    "data = [parseArticleNode(article) for article in articleNodes]\n",
    "df = pandas.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
