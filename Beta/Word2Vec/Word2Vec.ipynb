{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Word2Vec with Python\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient.\n",
    "\n",
    "The Word2Vec tool takes a text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words. The resulting word vector file can be used as features in many natural language processing and machine learning applications. Word2Vec does not need labels in order to create meaningful representations. This is useful, since most data in the real world is unlabeled. If the network is given enough training data (tens of billions of words), it produces word vectors with intriguing characteristics. Words with similar meanings appear in clusters, and clusters are spaced such that some word relationships, such as analogies, can be reproduced using vector math. The most famous examples of highly trained word vectors are `\"king - man + woman = queen\"` and `\"Paris - France + Italy = Rome\"`.\n",
    " \n",
    "In Python, we will use the excellent implementation of Word2Vec from the [`gensim` package](https://pypi.python.org/pypi/gensim). If you don't alread y have gensim installed, you'll need to install it using pip\n",
    "\n",
    "    sudo pip3 install gensim\n",
    "    \n",
    "Although Word2Vec does not require graphics processing units (GPUs) like many deep learning algorithms, it is compute intensive. Both Google's version and the Python version rely on multi-threading (running multiple processes in parallel on your computer to save time). In order to train your model in a reasonable amount of time, you will need to install `cython` (instructions [here](http://docs.cython.org/src/quickstart/install.html)). Word2Vec will run without `cython` installed, but it will take days to run instead of minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset reading\n",
    "\n",
    "In this lesson, we will use a dataset for binary sentiment classification containing users reviews for movies from IMDb web site. You need download the dataset called <u><span style=\"color: red\">movie.zip (81.1Mb)</span></u> from [http://www.cs.cornell.edu](http://www.cs.cornell.edu/people/pabo/movie-review-data/) web site. It contains about 28,000 reviews in HTML format. \n",
    "\n",
    "After downloading the zip file unzip it in the folder with the current IPython notebook.\n",
    "\n",
    "Let's look at the conent of the some HTML file containg reviews to some movie. To parse HTML format and extract data between HTML tags we will use [`BeautifulSoup` Python library](http://www.crummy.com/software/BeautifulSoup/) wich we have used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read HTML file\n",
    "with open('polarity_html/movie/0002.html') as f:\n",
    "    html = f.read()\n",
    "# Create a new BeautifulSoup instance\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "# Display HTML file content if a prettified format\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each users review is wrapped in `<p>` HTML tag, but there are also some `<p>` tags with helpfull information, so we should miss them. But we can also see that `<p>` tags necessary for us are positioned after the first `<pre>` tag and before the last one. The similar content have all other HTML files with reviews (please check it). Thus, we will remain only those HTML text which is placed between these `<pre>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# HTML tags (including <pre></pre>) are written uppercase in dataset's files\n",
    "soup = BeautifulSoup(' '.join(html.split('</PRE>')[1:-1]))\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get text of reviews using [`findAll`](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num, i in enumerate(soup.findAll('p')):\n",
    "    print(num, '\\n', i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag-of-words mode of text representation\n",
    "\n",
    "Now we can get text of reviews from one HTML file and process then, but how do we convert them to some kind of numeric representation for machine learning, particularly, for text classification as good or bad review or for prediction of which movie was devoted the comment for? One common approach is called a **Bag-of-words**. The Bag-of-words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. For example, consider the following two sentences:\n",
    "\n",
    "    Sentence 1: \"The cat sat on the hat\"\n",
    "\n",
    "    Sentence 2: \"The dog ate the cat and the hat\"\n",
    "\n",
    "From these two sentences, our vocabulary is as follows:\n",
    "\n",
    "    { the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "To get our bags of words, we count the number of times each word occurs in each sentence. In `Sentence 1`, \"the\" appears twice, and \"cat\", \"sat\", \"on\", and \"hat\" each appear once, so the feature vector for `Sentence 1` is:\n",
    "\n",
    "    { the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "    Sentence 1: { 2, 1, 1, 1, 1, 0, 0, 0 }\n",
    "\n",
    "Similarly, the features for `Sentence 2` are: \n",
    "\n",
    "    Sentence 2: { 3, 1, 0, 0, 1, 1, 1, 1}\n",
    "    \n",
    "<img src=\"images/bag-of-words.png\" width=75%>\n",
    "    \n",
    "This vector representation does not preserve the order of the words in the original sentences. This kind of representation has several successful applications, for example email filtering.\n",
    "\n",
    "We'll be using the `feature_extraction` module from `scikit-learn` to create Bag-of-words features. If you have not `scikit-learn` installed, read [this](http://scikit-learn.org/stable/install.html) instruction.\n",
    "\n",
    "Before, we need to split a paragraph into sentences. There are all kinds of gotchas in natural language. English sentences can end with \"?\", \"!\", \"\"\", or \".\", among other things, and spacing and capitalization are not reliable guides either. For this reason, we'll use the Python [Natural Language Toolkit](http://www.nltk.org)'s punkt tokenizer for sentence splitting.  You'll need to [install](http://www.nltk.org/install.html) the library if you don't already have it on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = ''\n",
    "for i in soup.findAll('p'):\n",
    "    text += ' ' + i.text\n",
    "\n",
    "# Lowercase text, remove \"\\n\" symbols\n",
    "text = text.lower().replace('\\n', ' ').strip()\n",
    "    \n",
    "sentences = sent_tokenize(text)\n",
    "# Remove non-letters  \n",
    "sentences = map(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x).strip(), sentences)  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Creating the bag-of-words\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model and learns the vocabulary; \n",
    "# second, it transforms our training data into feature vectors. \n",
    "# The input to fit_transform should be a list of strings.\n",
    "data_features = vectorizer.fit_transform(sentences)\n",
    "l = vectorizer.get_feature_names()\n",
    "\n",
    "# Look at the collection of all words in `sentences`\n",
    "print(\"Unique words ({uniq}):\".format(uniq=len(l)))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "#data_features = data_features.toarray()\n",
    "\n",
    "#print(\"Data features size:\", data_features.shape)\n",
    "#print(\"Vector representation of sentences:\\n\")\n",
    "#for num, vec in enumerate(data_features):\n",
    "#    print(sentences[num], '\\n', vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data processing\n",
    "\n",
    "To train Word2Vec, it makes sense to remove punctuation. It also might be better not to remove numbers, but we will do it in our class to simplify training process. We need also to decide how to deal with frequently occurring words that don't carry much meaning. Such words are called \"stop words\"; in English they include words such as \"a\", \"and\", \"is\", and \"the\". Conveniently, there are Python packages that come with stop word lists built in. Let's import a stop word list from the Python NLTK. You need to install the data packages that come with it, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download text data sets, including stop words\n",
    "# You need download it only once. After this comment the next line\n",
    "# nltk.download() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `nltk` to get a list of stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the stop word list\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at which form all reviews will have after all transformations mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract HTML text wrapped in <p> tags\n",
    "text = ''   # Here we will collect all review in the current document\n",
    "for p in soup.findAll('p'):\n",
    "    text += ' ' + p.text\n",
    "# Look at how many words are in all reviews of this HTML file\n",
    "print(\"Total words amount:\", len(text.split()))\n",
    "# Remove non-letters  \n",
    "text = re.sub(\"[^a-zA-Z]\", \" \", text)  \n",
    "# Convert words to lowercase and split them  \n",
    "words = text.lower().split()  \n",
    "# Remove stop words from \"words\"\n",
    "words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "\n",
    "print(\"Without stopwords:\", len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gensim` only requires that the input must provide sentences sequentially, when iterated over. No need to keep everything in RAM: we can provide one sentence, process it, forget it, load another sentence…\n",
    "\n",
    "For example, if our input is strewn across several files on disk, with one sentence per line, then instead of loading everything into an in-memory list, we can process the input file by file or line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Import the stop word list\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#print(stopwords.words(\"english\"))\n",
    "\n",
    "class DataTransformer(object):  \n",
    "    \n",
    "    def __init__(self, dirname):  \n",
    "        self.dirname = dirname \n",
    "        \n",
    "    def __iter__(self):  \n",
    "        for fname in os.listdir(self.dirname):\n",
    "            # 1. Read the HTML file\n",
    "            with open(os.path.join(self.dirname, fname), encoding='latin-1') as f:\n",
    "                html = f.read()\n",
    "            # 2. Create a new BeautifulSoup instance\n",
    "            soup = BeautifulSoup(' '.join(html.split('</PRE>')[1:-1]), \"lxml\")\n",
    "            # 3. Extract HTML text wrapped in <p> tags\n",
    "            text = ''   # Here we will collect all review in the current document\n",
    "            for p in soup.findAll('p'):\n",
    "                text += ' ' + p.text\n",
    "            # 4. Remove non-letters  \n",
    "            text = re.sub(\"[^a-zA-Z]\", \" \", text)  \n",
    "            # 5. Convert words to lowercase and split them  \n",
    "            words = text.lower().split()  \n",
    "            # 6. Remove stop words from \"words\"\n",
    "            words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "            yield words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model building and saving\n",
    "\n",
    "With the list of nicely parsed words, we're ready to train the model. `gensim`’s Word2Vec API requires some parameters for initialization. Of course, they do have default values, but you want to define some on your own (note, below we list not all attributes):\n",
    "\n",
    "* `size` – denotes the number of dimensions present in the vectorial forms. If you have read the document and have an idea of how many 'topics' it has, you can use that number. For sizeable blocks, people use 100-200. \n",
    "\n",
    "* `min_count` – terms that occur less than min_count number of times are ignored in the calculations. This reduces noise in the semantic space. \n",
    "\n",
    "* `window` - only terms hat occur within a window-neighbourhood of a term, in a sentence, are associated with it during training. The usual value is 4. Unless your text contains big sentences, leave it at that.\n",
    "\n",
    "* `sg` – this defines the algorithm. If equal to 1, the skip-gram technique is used. \n",
    "\n",
    "* `min_count` - ignores all words with total frequency lower than this.\n",
    "        \n",
    "* `sample` - threshold for configuring which higher-frequency words are randomly downsampled; default is 1e-3, useful range is (0, 1e-5).\n",
    "\n",
    "* `workers` - defines how many worker threads to train the model (= faster training with multicore machines).\n",
    "       \n",
    "Choosing parameters is not easy, but once we have chosen our parameters, creating a Word2Vec model is straightforward.\n",
    "Next we want to initialize and train our model. Note that this will take some time (even a few hours depending on your computer's performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'boto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dd1f6d00d6fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# bring model classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoherencemodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhdpmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHdpModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mldamodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFakeDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldamodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaVowpalWabbit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLdaMallet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/wrappers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mldamallet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaMallet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdtmmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDtmModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mldavowpalwabbit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaVowpalWabbit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmart_open\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/smart_open/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msmart_open_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhttplib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mboto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murlsplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mboto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'boto'"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set values for various parameters  \n",
    "num_features = 100     # Word vector dimensionality                        \n",
    "min_word_count = 20    # Minimum word count                          \n",
    "num_workers = 64       # Number of threads to run in parallel  \n",
    "\n",
    "# Iterate all data\n",
    "sentences = DataTransformer('polarity_html/movie') \n",
    "\n",
    "# Let's measure the ellapsed time\n",
    "start = time.time()\n",
    "\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers = num_workers, \n",
    "                          size = num_features,\n",
    "                          min_count = min_word_count\n",
    "                         ) \n",
    "\n",
    "print(\"Elapsed time: {time}\".format(time = time.time() - start))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can **store/load** models using the standard `gensim` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model.save('IMDb_reviews.w2v_model')\n",
    "print(\"Model was saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load('IMDb_reviews.w2v_model')\n",
    "print(\"Model is loaded\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which uses `pickle` internally, optionally mmap‘ing the model’s internal large NumPy matrices into virtual memory directly from disk files, for inter-process memory sharing.\n",
    "\n",
    "If you don't plan to train the model any further, calling `init_sims()` will make the model much more memory-efficient.\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "In addition, you can load models created by the original C tool, both using its text and binary formats:\n",
    "\n",
    "    model = Word2Vec.load_word2vec_format('IMDb_reviews', binary=False)\n",
    "    # using gzipped/bz2 input works too, no need to unzip:\n",
    "    model = Word2Vec.load_word2vec_format('IMDb_reviews.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploring the model results\n",
    "\n",
    "Now that you have the model initialized, you can access all the terms in its vocabulary, using something like `list(model.vocab.keys())`. \n",
    "\n",
    "To get the vectorial representation of a particular term, use `model[term]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = list(model.vocab.keys())\n",
    "print(\"Words amount in Word2Vec vocabulary:\", len(vocab) )\n",
    "print(\"\\nThe first 10 words:\\n\", vocab[:10] )\n",
    "print('learn' in model.vocab )\n",
    "print(\"\\nVector represintation of 'learn':\\n\", model['learn'] )\n",
    "print(\"\\nThe size of 'learn':\", model['learn'].size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec supports several word similarity tasks out of the box. Particularly, you can find a word as an arithmetical combination of some words by its meaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=['jolie'], negative=['pitt'], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how good the model was trained we can get not exepted result (in the begining of this lesson we've provided a classical example of Word2Vec usage `\"king - man + woman = queen\"`) as it is above (note, if you are lucky you may get the \"queen\" at once).\n",
    "\n",
    "Let's check whether \"queen\" is in the Word2Vec vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'queen' in model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it is. So, we have shown only the most closest result in the example above. Let's display 50 matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.most_similar(positive=['woman', 'king'], negative=['man'], topn=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **retrain** model with new data to get more better results. Let's take a few fairy tails containg many combinations of \"king\" of \"queen\" words and read them like we've made above. You may find these documents in the folder \"fairy_tails\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FairyTails(object):  \n",
    "    \n",
    "    def __init__(self, dirname):  \n",
    "        self.dirname = dirname \n",
    "        \n",
    "    def __iter__(self):  \n",
    "        for fname in os.listdir(self.dirname):\n",
    "            # Read the TXT file\n",
    "            with open(os.path.join(dirname, fname)) as f:\n",
    "                txt = f.read()\n",
    "            # Process file content line by line\n",
    "            for line in txt:\n",
    "                # Remove non-letters and convert words to lowercase and split them\n",
    "                words = re.sub(\"[^a-zA-Z]\", \" \", line).lower().split()  \n",
    "                words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "                yield words\n",
    "\n",
    "fairy_tails = FairyTails('fairy_tails')\n",
    "\n",
    "# Let's measure the ellapsed time\n",
    "start = time.time()\n",
    "print \"Retraining model...\"  \n",
    "model.train(fairy_tails)\n",
    "print \"Elapsed time:\", time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if something change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, now my retrained model positioned \"queen\" in TOP 5 results.\n",
    "\n",
    "As we mentioned above, to get a wonderfull result we need train Word2Vec algorithm on the large dataset (with tens bilions of words, but the current dataset contains a few millions of words). Other way: shuffle documents and train algorithm many times on the same dataset, i.e. we can read our 30 000 HTML documents in various order a few decades or hundreds times and retrain model on each documents combination. It is a good practice, but it is a time-consuming process.\n",
    "\n",
    "A few examples, which work properly in my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.most_similar(positive=['paris', 'england'], negative=['france'], topn=1)\n",
    "\n",
    "print model.most_similar(positive=['woman', 'boy'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few other methods of Word2Vec using for text analysis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To get a list of most similar words\n",
    "print model.most_similar('good')\n",
    "\n",
    "print model.most_similar(\"earth\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the excess word in the sequence\n",
    "print model.doesnt_match([\"breakfast\", \"cereal\", \"dinner\", \"lunch\"])\n",
    "\n",
    "print model.doesnt_match(\"good fine ugly wonderfull\".split())\n",
    "\n",
    "# And more difficult variant\n",
    "print model.doesnt_match(\"theory study education science\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the measure of similarity of two words\n",
    "print model.similarity('woman', 'man')\n",
    "\n",
    "print model.similarity('beautiful', 'ugly')\n",
    "\n",
    "print model.similarity('leave', 'leaf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each word is a vector in 100-dimensional space, we can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review or in some sentence (for this purpose, we removed stop words, which would just add noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_word_to_matrix(model, sentence):\n",
    "    l = []\n",
    "    for word in sentence.lower().split():\n",
    "        try:\n",
    "            l.append(model[word])\n",
    "            print word\n",
    "        except:\n",
    "            pass\n",
    "    return np.array(l)\n",
    "\n",
    "def get_agg_vector(model, sentence):\n",
    "    word_array = transform_word_to_matrix(model, sentence)\n",
    "    return word_array.mean(axis=1)\n",
    "\n",
    "print get_agg_vector(model, \"The dog ate the cat and the hat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This representation of sentence (the average vectors) can be uses in a machine learning algorithm. \n",
    "\n",
    "At the end let's provide a simple example how we can visuaize obtained results and build a tree of a two levels hierarchy of most similar words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plot size\n",
    "plt.rcParams['figure.figsize'] = (17, 12)\n",
    "\n",
    "# Recursive function allowing to draw a tree using NetworkX\n",
    "def hierarchy_pos(G, root, width=1., vert_gap = 0.2, vert_loc = 0, xcenter = 0.5, \n",
    "                  pos = None, parent = None):\n",
    "    '''If there is a cycle that is reachable from root, then this will see infinite recursion.\n",
    "       G: the graph\n",
    "       root: the root node of current branch\n",
    "       width: horizontal space allocated for this branch - avoids overlap with other branches\n",
    "       vert_gap: gap between levels of hierarchy\n",
    "       vert_loc: vertical location of root\n",
    "       xcenter: horizontal location of root\n",
    "       pos: a dict saying where all nodes go if they have been assigned\n",
    "       parent: parent of this branch.'''\n",
    "    if pos == None:\n",
    "        pos = {root:(xcenter,vert_loc)}\n",
    "    else:\n",
    "        pos[root] = (xcenter, vert_loc)\n",
    "    neighbors = G.neighbors(root)\n",
    "    if parent != None:\n",
    "        neighbors.remove(parent)\n",
    "    if len(neighbors)!=0:\n",
    "        dx = width/len(neighbors) \n",
    "        nextx = xcenter - width/2 - dx/2\n",
    "        for neighbor in neighbors:\n",
    "            nextx += dx\n",
    "            pos = hierarchy_pos(G,neighbor, width = dx, vert_gap = vert_gap, \n",
    "                                vert_loc = vert_loc-vert_gap, xcenter=nextx, pos=pos, \n",
    "                                parent = root)\n",
    "    return pos\n",
    "\n",
    "# Create a new graph instance\n",
    "G=nx.Graph()\n",
    "\n",
    "# Define the first 5 most similar words\n",
    "main_word = 'earth'\n",
    "top5 = model.most_similar(main_word, topn=5)\n",
    "top5_words = map(lambda x: x[0], top5)\n",
    "print top5_words\n",
    "\n",
    "# To miss repetitions we will remind unique words\n",
    "unique = [i for i in top5_words]\n",
    "for word in top5_words:\n",
    "    G.add_edge(main_word, word)\n",
    "    for subword in model.most_similar(word, topn=3):\n",
    "        if subword[0] not in unique and subword[0] != main_word:\n",
    "            G.add_edge(word, subword[0])\n",
    "            unique.append(subword[0])\n",
    "\n",
    "        \n",
    "pos = hierarchy_pos(G, main_word)    \n",
    "nx.draw(G, pos=pos, with_labels=True, node_size=5000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
